{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: The mathematics of quantum mechanics\n",
    "\n",
    "  (2020) \n",
    "\n",
    "---\n",
    "\n",
    "#### After this lecture you will be able to:\n",
    "1. Describe the mathematical representation of quantum states\n",
    "2. Use advanced tensor tools from `numpy`\n",
    "3. Visualize qubits on the Bloch sphere\n",
    "4. Reason about wave functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Preview of Lecture 1:***\n",
    "_Today we introduce quantum **States**: the first of the three pillars of Quantum Mechanics (States, Transformations and Measurements). Quantum states are unit vectors in a complex vector space and they describe how a quantum system \"is\". As quantum states are vectors, they can be linearly combined to form other valid states (which are referred to in popular accounts of quantum mechanics as \"superpositions\").\n",
    "Qubits are the simplest possible quantum systems because their Hilbert space is only 2-dimensional, and thanks to this, their states can be visualized on a sphere (the Bloch sphere). More complex systems have states that are defined on higher-dimensional spaces, even infinite-dimensional spaces (in which case it makes more sense to talk about wavefunctions, which are the continuum version of state vectors)._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. It's all linear\n",
    "\n",
    "Quantum mechanics is an application of linear algebra. When you find you're struggling, you should remind yourself that \"it's just linear algebra!\".\n",
    "\n",
    "In QM we have three main classes of objects: States, Transformations and Measurements. All of them can be described by vectors, matrices, and/or higher rank tensors. So no matter how complicated things may seem, it's all just linear algebra.\n",
    "\n",
    "In Lecture 1 (this lecture) we will present quantum states, in Lecture 2 we talk about measurements of quantm systems, and in Lecture 3 we will study transformations of quantum systems.\n",
    "\n",
    "### 1.1 Complex vector spaces\n",
    "One important aspect of QM is that the vector spaces that we use are complex, i.e. the entries of the vectors are complex numbers. To help with clarity, we can indicate our complex vector space as $\\mathbb{C}^n$ where we have fixed its dimension to $n$. We will encounter vector spaces of various dimension. Let's now see a couple of fundamental operations that we will often need in QM: inner products and norms\n",
    "\n",
    "First of all, our vector spaces are Hilbert spaces, which means that they come equipped with an inner product:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{v}, \\mathbf{w}\\rangle = \\sum_iv_i^*w_i\n",
    "$$\n",
    "\n",
    "notice that the vector $\\bf v$ is conjugated when computing the inner product. \n",
    "\n",
    "Secondly, the inner product can be used to define a norm (that is why all inner product spaces are also normed spaces):\n",
    "\n",
    "$$\n",
    "||\\mathbf{v}|| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v}\\rangle} = \\sqrt{\\sum_iv_i^*v_i}\n",
    "$$\n",
    "\n",
    "### 1.2 Dirac's notation\n",
    "There is a particular notation that has been adopted by the scientific community, known as Dirac's bra-ket notation. It comes from the alternative way of writing an inner product between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ as $\\langle \\mathbf{v}|\\mathbf{w}\\rangle$. If you interpret it as two objects coming together and \"sticking\" to each other, you can separate them into $\\langle \\mathbf{v}|$ and $|\\mathbf{w}\\rangle$ and use them as row and column vectors (more appropriately as covectors and vectors). We call them \"bra\" and \"ket\" because together they sound like the word \"bracket\". Whatever we write inside the ket (or a bra) is just a label: it's a name for that (co)vector, which sometimes carries useful information about the quantum state that it represents, like when we express the state of electromagnetic vacuum as $|0\\rangle$, meaning zero photons.\n",
    "\n",
    "As the vector space is complex, remember that to turn vectors and covectors into each other you also take the complex conjugate of the elements: $\\langle\\psi| = |\\psi\\rangle^\\dagger$ where the dagger symbol $\\dagger$ computes the \"hermitian conjugate\" (i.e. the conjugate transpose).\n",
    "\n",
    "We will make abundant use of inner products and norms. So let's begin our first activity by defining a couple of helper functions to compute inner products and norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 1: inner products and norms (10 min)\n",
    "- Write a function $f(\\mathbf{v}, \\mathbf{w})$ that computes the inner product between two vectors $\\mathbf{v}$ and $\\mathbf{w}$. The signature should be `f(array(complex), array(complex)) -> complex`\n",
    "- Write a function $f(\\mathbf{v})$ that computes the norm of a vector $\\mathbf{v}$. The signature should be `f(array(complex)) -> float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_prod(v, w):\n",
    "    return np.sum(np.conj(v) * w)\n",
    "\n",
    "def norm(v):\n",
    "    return inner_prod(v,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 A bit of useful numpy\n",
    "\n",
    "`Numpy` is one of the most popular python libraries for numerical calculus. We want to use it as much as possible, because the functions that you call actually run compiled C code, which is very fast.\n",
    "\n",
    "The most useful tool that I want to teach you is the function `np.einsum()`, but before we get there let's learn about tensors and axes. Tensors are generalizations of vectors and matrices. A tensor is an array of numbers, where each number is identifed by a set of coordinates, or indices. \n",
    "\n",
    "The meaning that we give to the tensor depends on the application that we have in mind for that tensor. Here's a few examples:\n",
    "\n",
    "1. A complex tensor with 1 index of dimension $n$ can be interpreted as a vector $T_{i} \\in \\mathbb{C}^n$.\n",
    "\n",
    "2. A complex tensor with two indices of dimension $m$ and $n$ can be interpreted as a matrix, i.e. a map $T_{ij} : \\mathbb{C}^n\\rightarrow \\mathbb{C}^m$, but also as a vector $T_{ij} : \\in \\mathbb{C}^n\\otimes \\mathbb{C}^m$.\n",
    "\n",
    "A general tensor is simply an object with multiple indices: $T_{ijklmn\\dots}$, whose meaning depends from the context.\n",
    "\n",
    "The number of indices is called the _rank_ of the tensor, so vectors are rank-1 tensors, matrices are rank-2 tensors etc...\n",
    "\n",
    "Each index has a dimension (i.e. the number of integer values that it can have), and the dimension does not have to be the same for all the indices of a tensor. If we call $d(j)$ the dimension of the index $j$, then a tensor $T_{j_1,\\dots,j_r}$ of rank $r$ contains $d(j_1)\\times d(j_2)\\times\\dots\\times d(j_r)$ values in it, and so the size of a tensor scales exponentially with the number of indices, i.e. with the rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` has many useful pre-baked functions in order to deal with vectors and matrices, but it can also easily deal with higher-rank tensors. Here are a few useful functions and methods to know:\n",
    "\n",
    "- `np.newaxis`: this method allows us to create new indices for a tensor\n",
    "- `Ellipsis`: this object is a placeholder for any number of indices \n",
    "- Broadcasting: this automatic operation allows us to define computations between tensors of incompatible shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of np.newaxis\n",
    "\n",
    "v = np.array([1,2,3]) # a vector\n",
    "\n",
    "v1 = v[:, np.newaxis]\n",
    "print(f'Adding a new second index. New shape of the tensor is {v1.shape}:\\n', v1, '\\n')\n",
    "\n",
    "v2 = v[np.newaxis, :]\n",
    "print(f'Adding a new first index. New shape of the tensor is {v2.shape}:\\n', v2, '\\n')\n",
    "\n",
    "v3 = v[:, np.newaxis, np.newaxis]\n",
    "print(f'Adding a new second and third index. New shape of the tensor is {v3.shape}:\\n', v3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new axes for broadcasting\n",
    "\n",
    "v = np.array([1,2,3])\n",
    "m = np.ones((3,3)) # a 3x3 matrix\n",
    "\n",
    "# compare:\n",
    "\n",
    "print('broadcasting along 1st index (rows):')\n",
    "print(v[np.newaxis, :] * m, '\\n')\n",
    "\n",
    "print('broadcasting along 2nd index (columns):')\n",
    "print(v[:, np.newaxis] * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of ellipsis\n",
    "\n",
    "T = np.zeros((2,3,4,5)) # tensor of shape (2,3,4,5)\n",
    "T[..., np.newaxis].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `np.einsum()`\n",
    "Now we can talk about `np.einsum()`, which is an extremely useful function. It can handle all sorts of tensor products, transposition, traces and much more.\n",
    "\n",
    "It takes as first argument a string that explains what happens to the indices (indicated as letters) and then it takes as many tensors as required by the string. There is only one rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{Repeated\\ indices\\ are\\ summed\\ over}\n",
    "$$\n",
    "\n",
    "This means that if we use the same symbol (the same letter) for an index, we mean that in the expression there should be a summation over that index. Let's see a few examples.\n",
    "\n",
    "#### Matrix multiplication\n",
    "Matrix multiplication is:\n",
    "\n",
    "$$\n",
    "(MN)_{ik} = \\sum_j M_{ij}N_{jk} \\equiv M_{ij}N_{jk}\n",
    "$$\n",
    "\n",
    "In the last step we use what is called \"Einstein's summation convention\", where we omit writing the summation symbol $\\sum_j$, because $j$ is repeated (it appears in both $M$ and $N$) and therefore it's clear that it's being summed over. When we sum over a repeated index we say that we _contract that index_.\n",
    "\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ik', M, N)\n",
    "```\n",
    "\n",
    "Matrix multiplication between, say, 4 matrices is\n",
    "$$\n",
    "(MNPQ)_{im} = \\sum_{jkl} M_{ij}N_{jk}P_{kl}Q_{lm} \\equiv M_{ij}N_{jk}P_{kl}Q_{lm}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk,kl,lm -> im', M, N, P, Q)\n",
    "```\n",
    "\n",
    "#### Higher rank contractions\n",
    "This can obviously work for tensors of any rank! Here's a silly example that does not mean anything:\n",
    "\n",
    "$$\n",
    "T_{m} = \\sum_{jkl} M_{j}N_{jklm}P_{jk}Q_{l} \\equiv M_{j}N_{jklm}P_{jk}Q_{l}\n",
    "$$\n",
    "\n",
    "(note that $m$ is the only index here which is never repeated, so the final result must be a vector indexed by $m$)\n",
    "\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('j,jklm,jk,l -> m', M, N, P, Q)\n",
    "```\n",
    "\n",
    "#### Transposition\n",
    "The string after the arrow allows us to do some final rearranging of the indices, like a transposition:\n",
    "\n",
    "$$\n",
    "(MN)^T = (\\sum_{j} M_{ij}N_{jk})^T \\equiv (M_{ij}N_{jk})^T\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,jk -> ki', M, N) # notice: ki and not ik\n",
    "```\n",
    "\n",
    "#### Traces\n",
    "If we repeat an index belonging to the same tensor, we compute a trace:\n",
    "\n",
    "$$\n",
    "Tr(M) = \\sum_{i} M_{ii} \\equiv M_{ii}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ii', M)\n",
    "```\n",
    "\n",
    "#### Tensor products (i.e. outer products)\n",
    "Outer products are the opposite of inner products, i.e. we simply don't contract indices:\n",
    "\n",
    "E.g. with vectors:\n",
    "$$\n",
    "\\mathbf{v}\\otimes \\mathbf{w} = v_{i}w_{j}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('i,j -> ij', v, w)\n",
    "```\n",
    "\n",
    "Or with matrices:\n",
    "$$\n",
    "M\\otimes N = M_{ij}N_{kl}\n",
    "$$\n",
    "With `np.einsum` this would be:\n",
    "\n",
    "```python\n",
    "np.einsum('ij,kl -> ijkl', M, N)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 2: A better Hilbert-Schmidt inner product (10 min)\n",
    "\n",
    "The Hilbert-Schmidt inner product is an inner product between complex matrices and it is defined as follows:\n",
    "\n",
    "$$\n",
    "\\langle M, N\\rangle = Tr(M^\\dagger N)\n",
    "$$\n",
    "\n",
    "where $M^\\dagger$ means we transpose and complex-conjugate $M$.\n",
    "\n",
    "- `v1`: implement the formula as is written above, using `np.matmul`, `np.trace` and `np.conj` to compute the conjugate\n",
    "- `v2`: implement the formula using `np.einsum` and `np.conj`\n",
    "- `v3` (Bonus): implement the formula in a more efficient way than `v1` without using `np.einsum` (tip: use what you learned from `v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(np.matmul(np.transpose(np.conj(M)), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum('ji,ji', np.conj(M), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.conj(M)*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 3: A better way to multiply by a diagonal matrix (10 min)\n",
    "\n",
    "Consider the product between three matrices: $ABC$, where $B$ is a diagonal matrix.\n",
    "\n",
    "- `v1`: implement this product as is written above (treat $B$ as if it were a regular matrix) using `np.einsum`\n",
    "- `v2`: implement this product in a more efficient way (use the fact that $B$ is diagonal) using `np.einsum` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum('ij,jk,kl -> il', A, B, C) # just 3 matrices\n",
    "\n",
    "np.einsum('ij,jj,jl -> il', A, B, C) # B is diagonal\n",
    "\n",
    "np.einsum('ij,j,jl -> il', A, np.diag(B), C) # only using the diagonal of B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantum systems and their properties\n",
    "Quantum systems are usually very fundamental objects (electrons, atoms, photons, etc...). This is because objects behave according to quantum mechanics when they are simple and isolated. We will see in a upcoming lecture that as systems interact with each other their quantum behaviour washes out and they look like classical systems. The state of a quantum system is a mathematical object that allows us to make predictions about the systems, such as \"what will the result of this measurement be?\", or \"how will the state change if I apply this transformation?\", or \"how similar are these states?\" and so on.\n",
    "\n",
    "### 2.1 Properties $\\leftrightarrow$ Hilbert spaces\n",
    "**Axiom 1: To each property of a system we associate a Hilbert space.**\n",
    "\n",
    "Once we use enough Hilbert spaces to cover all of the properties of a system, we can say that we have a complete description of this system. \n",
    "We describe a property by specifying its \"state\", and the state is a vector in the Hilbert space associated with the property.\n",
    "\n",
    "NOTE: _Often we will refer to this vector as the state of the system itself even though we are actually talking about just one of its properties. This is okay as it's also done in everyday language ('the cat is black' is an acceptable sentence to describe a cat even if it doesn't specify its name, its position, etc...)._\n",
    "\n",
    "The dimension of these Hilbert spaces should equal the number of _perfectly distinguishable values_ that the property can have. \n",
    "\n",
    "NOTE: _This is where we begin noticing a difference between classical physics and quantum physics: in classical physics the values of a property are always distinguishable (i.e. a chair is either here or over there, and we can know where it is by looking at it), but in quantum physics the values of a property may not be always distinguishable from each other. To fully understand this we will need Born's rule (Lecure 2), so for the time being hold on to this thought._\n",
    "\n",
    "We said that a quantum state is represented by a vector in a Hilbert space. However, not every vector in $\\mathbb{C}^n$ is a valid quantum state. Quantum states are only the vectors of norm 1: $|\\psi\\rangle$ is a quantum state iff $\\langle\\psi|\\psi\\rangle = 1$.\n",
    "For reasons that will become clear in Lecture 2 when we introduce measurements, we can define a set of \"perfectly distinguishable\" states by using an orthonormal basis (such as the canonical basis). For example, we can describe the spin of an electron (which has two perfectly distinguishable values) within the Hilbert space $\\mathbb{C}^2$, using the basis $\\binom{1}{0}$ and $\\binom{0}{1}$ to indicate two distinguishable states (e.g. the spin pointing in two opposite directions).\n",
    "\n",
    "We will indicate each element of the standard basis in Dirac's notation simply by numbering the kets: $|0\\rangle, |1\\rangle, |2\\rangle, \\dots, |n-1\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 4: normalize vectors and generate the canonical basis elements (10 min):\n",
    "- Write a function $f(\\mathbf{v})$ which takes a vector $\\mathbf{v}$ and normalizes it. The signature should be `f(array(complex)) -> array(complex)`.\n",
    "- Write a function $f(dim, k)$ which returns the $k$-th standard basis element in a Hilbert space of dimension $dim$. The signature should be `f(int, int) -> array(complex)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return v/np.linalg.norm(v)\n",
    "\n",
    "def basis(dim, k):\n",
    "    z = np.zeros(dim, dtype=np.complex64)\n",
    "    z[k] = 1.0 + 0.0j\n",
    "    return z\n",
    "\n",
    "def basis(dim, k): # alternative, but not as efficient\n",
    "    return np.identity(dim, dtype=np.complex64)[k] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 State superpositions\n",
    "A fundamental property of vector spaces is the possibility to take linear superpositions of vectors. This can be also done with quantum states, as they are vectors. This is one of the central features of QM, which definitely sets it apart from classical physics! But what does it mean for the values of a property to be in superposition?\n",
    "\n",
    "It means that in the quantum world there are many more \"_ways of being_\" than there are classically. In the classical world a cat is either dead or alive, but in the quantum world it can really be both at the same time, and not just both, there is an infinite number of ways of being both.\n",
    "\n",
    "The most general state of a property is a linear combination of all the elements associated to the canonical basis:\n",
    "\n",
    "$$\n",
    "|\\psi\\rangle = \\sum_{i=0}^{n-1}\\psi_i|i\\rangle \\quad\\leftrightarrow \\quad(\\psi_0, \\psi_1,\\dots,\\psi_{n-1})^T \\in \\mathbb{C}^n\n",
    "$$\n",
    "\n",
    "We call the complex numbers $\\psi_i$ the \"probabiliy amplitudes\" for a reason that will be clear in a moment.\n",
    "\n",
    "Obviously it must hold that $\\langle\\psi|\\psi\\rangle = 1$ otherwise $|\\psi\\rangle$ would not be state. At the level of amplitudes this implies:\n",
    "\n",
    "$$\n",
    "\\langle\\psi|\\psi\\rangle = \\sum_{i=0}^{n-1}\\psi_i^*\\psi_i = \\sum_{i=0}^{n-1}|\\psi_i|^2 = 1\n",
    "$$\n",
    "\n",
    "So we can interpret the set of real numbers $\\{|\\psi_i|^2\\}$ as a set of probabilities (becasue they sum to 1). This is why the complex values $\\psi_i$ are called \"probabiliy amplitudes\": they are not exactly probabilities, but their squared absolute value is.\n",
    "\n",
    "There is a deeper reason why we talk about probabilities (not just that we have a bunch of numbers that sum to 1), but this has to wait for Lecture 2, when we will talk about measurements and the probabilities associated to the various outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 5: state superpositions, random states (10 min)\n",
    "- Write a function $f(d)$ that returns a random $d$-dimensional quantum state. The signature should be `f(int) -> array(complex)`\n",
    "- Write a function $f(d)$ that returns a random $d$-dimensional quantum state with a flat probability distribution (i.e. the absolute value squared of all its amplitudes is constant). The signature should be `f(int) -> array(complex)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_state(d):\n",
    "    x = np.random.normal(size=d)\n",
    "    y = np.random.normal(size=d)\n",
    "    return normalize(x + 1j*y)\n",
    "\n",
    "def rand_state_const(d): # a random state with constant probabilities\n",
    "    phases = np.exp(2*np.pi*1j*np.random.rand(d))\n",
    "    return normalize(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qubits\n",
    "Qubits are the simplest quantum states possible. They are systems with only a single property (so a single Hilbert space is suffcient to describe them completely) and this property has only two distinguishable values (so the Hilbert space has dimension 2).\n",
    "\n",
    "A general qubit state is characterized by just two complex amplitudes:\n",
    "\n",
    "$$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle \\quad \\leftrightarrow\\quad \\binom{\\alpha}{\\beta}$$\n",
    "\n",
    "Something that we can do with qubits that is not possible with higher-dimensional states is the possibility to visualize them explicitly. Despite depending on two complex numbers (and therefore having 4 real degrees of freedom), the fact that we impose normalization and the fact that the global phase does not matter (we'll prove this in Lecture 2) brings down the number of degrees of freedom from 4 to just 2. Not only that, but the topology of the state space turns out to be equivalent to a sphere (technically, it's the space $\\mathbb{C}\\mathcal{P}^1$, i.e. the complex projective line, which is isomorphic to a 2-sphere).\n",
    "\n",
    "So any qubit state corresponds to a point on a sphere, that's convenient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install qutip if you haven't already done it\n",
    "!pip install qutip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qutip import Bloch, Qobj\n",
    "b = Bloch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.add_states([Qobj(rand_state(2)) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 6: Visualize the quantum relative phase (10 min)\n",
    "- Produce a Bloch sphere visualization of a set of qubit states in the form $|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{e^{i\\phi}}{\\sqrt{2}}|1\\rangle$ for varying values of the phase $\\phi$. All of these states have an equal \"amount\" of $|0\\rangle$ and $|1\\rangle$, but their relative phase is different and this phase leads to physically distinct states.\n",
    "- Using the parametrization $\\sin(\\theta/2)|0\\rangle + e^{i\\phi}\\cos(\\theta/2)|1\\rangle$, fill the Bloch sphere with three sets of qubit states that span three large circles on the XY, YZ and ZX planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit(theta, phi):\n",
    "    return Qobj(np.array([np.sin(theta/2), np.exp(1j*phi)*np.cos(theta/2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Bloch()\n",
    "b.add_states([qubit(np.pi/2, phi) for phi in np.linspace(0, 2*np.pi, 20)])\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.clear()\n",
    "b.add_states([qubit(theta, 0) for theta in np.linspace(0, 2*np.pi, 20)])\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.clear()\n",
    "b.add_states([qubit(theta, np.pi/2) for theta in np.linspace(0, 2*np.pi, 20)])\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High-dimensional states: wave functions\n",
    "Some properties have a clearly finite-dimensional space associated to them, such as the spin of an electron which as we saw, is only 2-dimensional. Other properties however, have an actually _continuous_ range of values, such as the position of a particle. In theoretical QM this is treated using infinite-dimensional Hilbert spaces (e.g. $L_2(\\mathbb{R})$, the space of mod-square integrable functions). This however is not the best for us because we cannot fit an infinite-dimensional vector space in our computer. There are a couple of ways out of this issue:\n",
    "\n",
    "1. Discretize the space and things will be okay in the limit for a discretization that is small enough\n",
    " - pro: easy\n",
    " - con: states that not physically important are treated as fundamental\n",
    " \n",
    " \n",
    "2. Use a discrete (still finite) basis of continuous functions\n",
    " - pro: physically relevant states are fundamental\n",
    " - con: a bit harder to set up\n",
    "\n",
    "In both cases, we approximate an infinite-dimensional vector space with a finite-dimensional one. The clever way to think about this is that not all quantum states are equally likely/important/useful and so not all finite-dimensional approximations are equally good, even for equal dimensionality. We should pick the one that best suits our needs.\n",
    "\n",
    "This is where wave functions come in. A wave function is an element of an infinite-dimensional Hilbert space. You can think of it as a continuous version of a vector:\n",
    "\n",
    "$$\n",
    "|\\psi\\rangle = \\int_{-\\infty}^{\\infty}\\psi(x)|x\\rangle\\, dx\n",
    "$$\n",
    "\n",
    "The way to think about it is that in the finite dimensional case, the entries of a state vector were indexed by the little index $i$ below each amplitude $\\psi_i$, whereas here they are indexed by a continuous index $x$. After all, a function $\\psi(x)$ can be thought of as a \"vector of values\", indexed by a continuous index $x$.\n",
    "\n",
    "So the inner product in this infinite-dimensional vector space is\n",
    "\n",
    "$$\n",
    "\\langle\\psi|\\phi\\rangle = \\int_{-\\infty}^{\\infty}\\psi(x)^*\\phi(x)dx\n",
    "$$\n",
    "\n",
    "and therefore the norm is:\n",
    "\n",
    "$$\n",
    "\\langle\\psi|\\psi\\rangle = \\int_{-\\infty}^{\\infty}|\\psi(x)|^2\\,dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 First solution: brute-force discretization\n",
    "The first solution is equivalent to clipping the space from $(-\\infty, \\infty)$ to a finite interval, say $[0,L]$ and dividing it into steps of length $\\delta x$. Then we obtain a Hilbert space of dimension $L/\\delta x$. This is like having a particle in a 1D box of length $L$.\n",
    "\n",
    "All the states are going to be defined in this Hilbert space. For example, the state of a particle that sits at the origin is $|\\psi\\rangle=(1,0,0,\\dots)$, the particle that sits at position $\\delta x$ is $|\\psi\\rangle = (0,1,0,0,\\dots)$ and so on.\n",
    "\n",
    "This is the high-dimensional equivalent of the canonical basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 7: wave function visualization (10 min)\n",
    "(use the helper function `plot_position()` that you will find in the library of the course)\n",
    "\n",
    "- Let $L = 100$. Visualize a random state\n",
    "- Let $L = 10$. Visualize 3 _different_ states where the particle has an equal probability of being at x = 0 and x = 3.\n",
    "- Let $L = 100$. Visualize a state whose probability distribution is a Gaussian function centered at 50 with std = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from INFPHY201 import plot_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_position(rand_state(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = np.array([1/np.sqrt(2),0,0,np.exp(1j*s)*1/np.sqrt(2),0,0,0,0,0,0])\n",
    "\n",
    "plot_position(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = np.array([np.exp(-(x-50)**2/(2*25)) for x in range(100)])\n",
    "\n",
    "psi = psi/np.linalg.norm(psi)\n",
    "\n",
    "plot_position(psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Second solution: basis of functions\n",
    "\n",
    "When we form superpositions of quantum states, we always use the same formula:\n",
    "\n",
    "$$\n",
    "|\\psi\\rangle = \\sum_i \\psi_i|i\\rangle\n",
    "$$\n",
    "\n",
    "so what if we use a different basis? Instead of the canonical basis $\\{|i\\rangle\\}$ we could use $\\{|v_i\\rangle\\}$, as long as it is orthonormal:\n",
    "\n",
    "$$\n",
    "\\langle v_i|v_j\\rangle = \\delta_{ij}\n",
    "$$\n",
    "\n",
    "Here is where we can get creative. We can rotate the canonical basis to any other basis by using a unitary matrix $V$:\n",
    "\n",
    "$$\n",
    "|v_i\\rangle = V|i\\rangle\n",
    "$$\n",
    "\n",
    "In fact, if we apply $V$ to the canonical basis elements $|i\\rangle$, the vectors $|v_i\\rangle$ are the columns of $V$, and we know that the columns of a unitary matrix are orthonormal vectors (and so are the rows, by the way).\n",
    "\n",
    "But we have so many possible bases to choose from! How do we pick one?\n",
    "To answer this question we need to know a bit more about Quantum Mechanics, in particular about the physical importance of the energy of a system: physical systems tend to prefer lower energy states so we should make sure we can describe those first, then we can progressively describe higher-energy states until we are satisfied or we run out of resources. Also, if we have a good description of states with a fixed energy, we will see in Lecture 3 that it will become really easy to predict how a system evolves in time.\n",
    "\n",
    "NOTE: _To those of you who have studied compression, the following may feel familiar. When you want to describe something the first thing you do is you look at what occurs more often, and you make sure you can describe that well, then you go progressively toward things that occur less often until you are happy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from INFPHY201 import psi_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x,y = psi_n(1, 200, 3)\n",
    "plt.plot(x, np.abs(y)**2); # plotting the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 8: Orthogonality of wave functions (10 min)\n",
    "- Verify that the elements of the basis of eigenfunctions of the square box are orthonormal (pick a large discretization, say $d=100$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = np.array([psi_n(L=1, dim=100, n=n)[1] for n in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.real(basis@np.conj(basis.T)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
